{"cells":[{"cell_type":"markdown","source":["#**Face Recognition Using Convolutional Neural Network**\n\n- --------------"],"metadata":{}},{"cell_type":"markdown","source":["##Work FLow\n- -----\n###I. Import Libiaries \nYou can run this notebook on databricks, after create libraries: *keras*, *opencv-python*.\nOr, you can run this notebook locally. Anaconda is a good choice."],"metadata":{}},{"cell_type":"code","source":["import os\nimport sys\nimport numpy as np\nimport cv2\nimport random\nimport numpy as np\nimport keras\nfrom sklearn.cross_validation import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\nfrom keras.models import load_model\nfrom keras import optimizers\nfrom keras import backend as K\nimport h5py \nfrom keras.models import model_from_json \nimport matplotlib.pyplot as plt\nimport pandas as pd"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["###II. Image Processing\nThe input shape of a CNN has rule, so before build our model, we need to process image dataset. We want to read graphs one by one, normalization, and add different labels to each of them according to the person shows on it, like ‘0’ for A and ‘1’ for B. \n\nWe use OpenCV, which has an image processing module that includes linear and non-linear image filtering, geometrical image transformations (resize, affine and perspective warping, generic table-based remapping), color space conversion, histograms, and so on. to adjust our images."],"metadata":{}},{"cell_type":"markdown","source":["![image processing](https://raw.githubusercontent.com/JunxiFan/Big-Data-Systems-and-Intelligence-Analytics/master/image_portfolio/image_processing.jpg)\n\nWhen we read an image, suppose we have x pixel wide and y height image. OpenCV can read an image file and coverts it to a list (width × height × 3, 3 means three channels of colors as red, green, blue). To normalize the input data shape of our neural network model, for one images, we compare the width and height, and add borders to the shorter sides. At last, we resized the picture to 32 × 32 pixels format."],"metadata":{}},{"cell_type":"code","source":["IMAGE_SIZE = 64\n\ndef resize_image(image, height_output = IMAGE_SIZE, width_output = IMAGE_SIZE):\n     # initialize border's value. default value of four borders is 0\n    top, bottom, left, right = (0, 0, 0, 0)\n    \n    #get image's size\n    width, height, _ = image.shape\n    \n    #find the longer side of the image\n    longer_side = max(width, height)    \n    \n    #calculate how many we should add to the shorter side\n    if width < longer_side:\n        dh = longer_side - width\n        top = dh // 2\n        bottom = dh - top\n    elif height < longer_side:\n        dw = longer_side - height\n        left = dw // 2\n        right = dw - left\n    else:\n        pass \n    \n    #RGB\n    BLACK = [0, 0, 0]\n\n    # add border to make two sides the same. \"cv2.BORDER_CONSTANT \" is the color of border, configured by \"value\"\n    constant = cv2.copyMakeBorder(image, top , bottom, left, right, cv2.BORDER_CONSTANT, value = BLACK)\n    \n    return cv2.resize(constant, (height_output, width_output))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["How to read hundreds of images with labels into our project is the next problem. Now we have many graphs of two people, for example, Johnny Depp and Natalie Portman, separated in two folders.  After doing this classification, we can get a list of images and a list of related labels."],"metadata":{}},{"cell_type":"code","source":["#read data into ram\nimages = []\nlabels = []\ndef read_path(path_url):    \n    for dir_item in os.listdir(path_url):\n        full_path = os.path.abspath(os.path.join(path_url, dir_item))\n        \n        if os.path.isdir(full_path):    #if it is a folder, continue. (here a recursion is used)\n            read_path(full_path)\n        else: \n            if dir_item.endswith('.jpg'):\n                image = cv2.imread(full_path)                \n                image = resize_image(image, IMAGE_SIZE, IMAGE_SIZE)\n                images.append(image)                \n                labels.append(path_url)                                \n                    \n    return images,labels\n    \n#main function, read data\ndef load_dataset(path_url):\n    images,labels = read_path(path_url)\n  \n    #change images to 4-dimensions array\n    images = np.array(images)\n#     print(images.shape,\"total \")\n    \n    labels = np.array([0 if label.endswith('johnny_depp') else 1 for label in labels])    \n    return images, labels"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["We will do cross validation after training, so we split our dataset into three parts: Training, validation, and test sets. \n\nAlso, as it is a project about recognition, it’s necessary to do one-hot encoding to the labels. Label sets' shape will be a 2 dimention list, depends on the value of *nb_classes*. \n\nNext, change data type to float32 and normalize the value of RGB between 0-1, to improve network convergence speed, reduce training time, and reduce the value of training error.\n\nFor now, the preparation function before building CNN is complete."],"metadata":{}},{"cell_type":"code","source":["class Dataset:\n    def __init__(self, path_url):\n        #train data\n        self.train_images = None\n        self.train_labels = None\n        \n        #validate data\n        self.valid_images = None\n        self.valid_labels = None\n        \n        #test data\n        self.test_images  = None            \n        self.test_labels  = None\n        \n        #path of dataset\n        self.path_url = path_url\n        \n        #tensorflow (channels,rows,cols) or theano (rows,cols,channels)\n        self.input_shape = None\n        \n    # load dataset, then seperate dataset according to cross-validation\n    def load(self, nb_classes = 2):\n        #load dataset to ram\n        images, labels = load_dataset(self.path_url)        \n        \n        # split into three sets randomly\n        train_images, valid_images, train_labels, valid_labels = train_test_split(images, labels, test_size = 0.3, random_state = random.randint(0, 100))\n\n        _, test_images, _, test_labels = train_test_split(images, labels, test_size = 0.5, random_state = random.randint(0, 100))\n\n        self.input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)            \n            \n        #the amount of three data parts\n        print(train_images.shape[0], 'train samples')\n        print(valid_images.shape[0], 'valid samples')\n        print(test_images.shape[0], 'test samples')\n        \n        # label ===(one-hot encoding)===> 2 dimensions data\n        train_labels = np_utils.to_categorical(train_labels, nb_classes)                        \n        valid_labels = np_utils.to_categorical(valid_labels, nb_classes)            \n        test_labels = np_utils.to_categorical(test_labels, nb_classes)\n        \n        #image to float\n        train_images = train_images.astype('float32')            \n        valid_images = valid_images.astype('float32')\n        test_images = test_images.astype('float32')\n            \n        # normalize the value of RGB between 0-1\n        train_images /= 255\n        valid_images /= 255\n        test_images /= 255            \n        \n        self.train_images = train_images\n        self.valid_images = valid_images\n        self.test_images  = test_images\n        self.train_labels = train_labels\n        self.valid_labels = valid_labels\n        self.test_labels  = test_labels"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Next is load data from storage system. We have uploaded data source on DBFS (1062 jpg of Johnny Depp, 840 jpg of natalie portman), and read them here.\n\nAfter creating a new dataset object, including train_images, valid_images, test_images, train_labels, valid_labels, test_labels."],"metadata":{}},{"cell_type":"code","source":["dataset = Dataset('/dbfs/FileStore/tables/data/')    \ndataset.load()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["##III. Build CNN model\nKeras provides plenty of API for building neural network model. We can build a sequential convolutional neural network easily. For more details you can read [Keras official document layers part](https://keras.io/layers/about-keras-layers/)."],"metadata":{}},{"cell_type":"code","source":["def build_model(dataset, nb_classes = 2):\n    model = Sequential() \n\n    model.add(Conv2D(32, (3, 3), border_mode='same', \n                                 input_shape = dataset.input_shape))\n\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))        \n    # droupout: present from Overfitting\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(256))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(nb_classes))\n    model.add(Activation('softmax'))\n    model.summary()\n    \n    return model"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["here is an introduce of each kinds of layers in the CNN.\n\n###Convolution layer:\nThe emphasis here is on the function: Conv2D(). According to the Keras official document, 2D represents a 2D convolution, whose function is to perform sliding window convolution calculation on 2D input. Our facial image size is 64 x 64 pixels, which only contain length and width, so we are using a 2D convolution function to calculate the convolution. The sliding window calculation represents uses the convolution kernel to calculate pixels one by one in order.\n\n![conv2d img](https://raw.githubusercontent.com/JunxiFan/Big-Data-Systems-and-Intelligence-Analytics/master/image_portfolio/conv_layer.jpg)\n \nFirst, we will focus the convolution kernel on the first pixel of the image, here the pixel with a pixel value of 237. The area covered by the convolution kernel, and all pixels below it are averaged and then added together: \n\n_C(1) = 0 * 0.5 + 0 * 0.5 + 0 * 0.5 + 0 * 0.5 + 237 * 0.5 + 203 * 0.5 + 0 * 0.5 + 123 * 0.5 + 112 * 0.5_\n\nThen replace the first pixel in the image, then calculate the second, third…until we get a same size but convoluted image. As to the edge pixels, we fill over edge part with 0. We can use this setting in Conv2D() by code:\n\n_model.add(Conv2D(64, (3, 3), border_mode='same', input_shape = dataset.input_shape))_\n\nWe also need to tell Keras the data we input, which is 64*64 in RGB color, code: input_shape(64, 64, 3)\n\n###Activate function layer:\nRelu (Rectified Linear Units) function, the input is less than 0, the output is all 0, greater than 0 is equal to the input and output. The advantage of this function is its fast convergence. The keras library also supports several other activation functions: Softplus, Softsign, Tanh, Sigmoid, Hard_sigmoid, Linear. We tried all of this activation functions and decide to use Relu.\n\n###Max Pooling Layer:\nThe purpose of the pooling layer is to reduce the input feature map, simplify network computational complexity, and simultaneously compress features, highlighting key features.\n\n ![conv2d img](https://raw.githubusercontent.com/JunxiFan/Big-Data-Systems-and-Intelligence-Analytics/master/image_portfolio/MaxPooling_layer.jpg)\n \nWe establish the pooling layer by calling the MaxPooling2D() function. This function uses the maximum pooling method. This method selects the maximum value of the coverage area as the main feature of the area to compose a new reduced feature map. Thus, we will get a 32*32-pixel image after pooling.\n\n###Dropout Layer:\nThe Dropout layer randomly disconnects a certain percentage of input neuron links and consciously reduces the model parameters, making the model simple to prevent overfitting. Dropout() function use float parameter from 0-1 to define drop out percentage.\n\n###Flatten layer:\nAfter many times of convolution, pooling, and Dropout, here you can enter the full connection layer for final processing. The fully connected layer requires that the input data must be one-dimensional, so we must “squash” the input data into one dimension.\n\n###Dense layer:\nThe role here is for classification or regression, which is classification. We define dense layer through the Dense() function. One of the required parameters of this function is the number of neurons, which it is to specify how many outputs the layer has. In our code, the first dense layer specifies 512 neurons, that is, retains 512 features output to the next layer.\n\n###Classification layer: \nThe goal of the dense layer is to complete our classification requirements: 0 or 1.\nself.model.add(Dense(nb_classes))\nself.model.add(Activation('softmax'))\nIn the first row of code, we define classification requirement number of neurons, which is 2 for us. And in next layer, we use Softmax() to finish final classification. From the perspective of classification, the greater the output value of the neuron, the greater the likelihood that its corresponding category is a real category. Therefore, after the Softmax(), the upper N inputs are mapped to N probability distributions, and the sum of the probabilities is 1. The highest probability is the model predicted by the model."],"metadata":{}},{"cell_type":"markdown","source":["Our structure of CNN model:"],"metadata":{}},{"cell_type":"code","source":["MODEL = build_model(dataset)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["##IV.\tTrainning\nNext is let the dataset we have prepared to train this model and do cross validations. We set the cost function as ‘categorical_crossentropy’, a typical function dealing with categorical project. The optimizer is SGD (Stochastic gradient descent optimizer), at the same time, Keras includes support for momentum, learning rate decay, and Nesterov momentum, which means we can take multiple advantages from them. Using SGD only, the decent direction depends on batch data completely, so add a momentum can keep original decent direction in some degree, make the optimizer works faster, but more stable. We set the epoch as 100, and at last we get a graph using spark DataFrame."],"metadata":{}},{"cell_type":"code","source":["def train(dataset, model, batch_size = 40, nb_epoch = 100):\n    # SGD: a  compiler; lr: learning rate\n    sgd = SGD(lr = 0.001, decay = 1e-6, momentum = 0.9, nesterov = True) \n    \n    model.compile(loss='categorical_crossentropy',\n                  optimizer=sgd,\n                  metrics=['accuracy'])   \n\n    history = model.fit(dataset.train_images, dataset.train_labels,\n                   batch_size = batch_size,\n                   nb_epoch = nb_epoch,\n                   validation_data = (dataset.valid_images, dataset.valid_labels),\n                   shuffle = True)\n\n    res = history.history\n    return res"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["res = train(dataset, MODEL)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["i=1\ntemp_epoch = []\n\nfor each in range(len(res['acc'])):\n#   print(i)\n  temp_epoch.append(i)\n  i+=1\nres['epoch'] = temp_epoch\n\ndf = pd.DataFrame(res)\nspark_df = spark.createDataFrame(df)\ndisplay(spark_df)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["display(spark_df)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["##Test\n- ---\nBefore evaluate our model, we need to realize the standards, or, conditions of face recognition research area. As we know there are many out-standing solution of face recognition, like Fisher Vector Faces, DeepFace, Fusion, FaceNet, etc. \n\n![conv2d img](https://raw.githubusercontent.com/JunxiFan/Big-Data-Systems-and-Intelligence-Analytics/master/image_portfolio/faceRec_method_acc1.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["Using test dataset we generate before to evaluate our training result. Keras provides an evaluate function, use trained model and test set as parameters, it will return the value of loss and accuracy."],"metadata":{}},{"cell_type":"code","source":["def evaluate(model, dataset):\n    score = MODEL.evaluate(dataset.test_images, dataset.test_labels, verbose = 1)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])\n#         print(type(score))\n    return score\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["evaluate(MODEL,dataset)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["The result accuracy of test set is 0.91. Considering the amount of samples and the time we cost for training, this result is not bad.."],"metadata":{}},{"cell_type":"markdown","source":["##References\n\n1. [http://cis.csuohio.edu/~sschung/CIS660/DeepFaceRecognition_parkhi15.pdf]\n\n2. [https://www.microsoft.com/en-us/research/project/msra-cfw-data-set-of-celebrity-faces-on-the-web/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fprojects%2Fmsra-cfw%2F]\n\n3.  [https://www.wikiwand.com/en/Databricks]\n\n4.  [https://docs.opencv.org/3.4.1/d1/dfb/intro.html]"],"metadata":{}},{"cell_type":"markdown","source":["##License"],"metadata":{}},{"cell_type":"markdown","source":["####The text in the document by &lt;JUNXI FAN, KAIXIN GAO&gt; is licensed under CC BY 3.0 [https://creativecommons.org/licenses/by/3.0/us/]\n\nTHE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE COMMONS PUBLIC LICENSE (\"CCPL\" OR \"LICENSE\"). THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.\n\nBY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. TO THE EXTENT THIS LICENSE MAY BE CONSIDERED TO BE A CONTRACT, THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS."],"metadata":{}},{"cell_type":"markdown","source":["####The code in the document by &lt;JUNXI FAN, KAIXIN GAO&gt; is licensed under the MIT License [https://opensource.org/licenses/MIT]\n\nCopyright &lt;2018&gt; &lt;JUNXI FAN, KAIXIN GAO&gt;\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":32}],"metadata":{"name":"Code Review_INFO7390_Face Recognition","notebookId":971216357823437},"nbformat":4,"nbformat_minor":0}
